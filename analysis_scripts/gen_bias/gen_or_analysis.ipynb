{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_single(sentences):\n",
    "    \"\"\"\n",
    "    Extracts parts of speech (adjectives, nouns, verbs) from a list of sentences.\n",
    "    Args:\n",
    "        sentences (list): List of sentences to process.\n",
    "    Returns:\n",
    "        tuple: Three Counter objects containing counts of adjectives, nouns, and verbs.\n",
    "    \"\"\"\n",
    "    adjectives, nouns, verbs = Counter(), Counter(), Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence.lower())\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives[token.text] += 1\n",
    "            elif token.pos_ == \"NOUN\":\n",
    "                nouns[token.text] += 1\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs[token.text] += 1\n",
    "    \n",
    "    return adjectives, nouns, verbs\n",
    "\n",
    "def extract_pos_multi(sentences):\n",
    "    \"\"\"\n",
    "    Extracts parts of speech (adjectives, nouns, verbs) from a list of sentences,\n",
    "    considering multi-word phrases.\n",
    "    Args:\n",
    "        sentences (list): List of sentences to process.\n",
    "    Returns:\n",
    "        tuple: Three Counter objects containing counts of adjectives, nouns, and verbs.\n",
    "    \"\"\"\n",
    "    adjectives, nouns, verbs = Counter(), Counter(), Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence.lower())\n",
    "        temp_noun, temp_adj, temp_verb = [], [], []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Capture multi-word noun phrases (compound nouns)\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                temp_noun.append(token.text)\n",
    "            else:\n",
    "                if temp_noun:\n",
    "                    noun_phrase = \" \".join(temp_noun)\n",
    "                    nouns[noun_phrase] += 1\n",
    "                    temp_noun = []\n",
    "            \n",
    "            # Capture adjective phrases (adverb + adjective)\n",
    "            if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n",
    "                temp_adj.append(token.text)\n",
    "            else:\n",
    "                if temp_adj:\n",
    "                    adj_phrase = \" \".join(temp_adj)\n",
    "                    adjectives[adj_phrase] += 1\n",
    "                    temp_adj = []\n",
    "            \n",
    "            # Capture verb phrases (auxiliary verbs + main verb)\n",
    "            if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "                temp_verb.append(token.text)\n",
    "            else:\n",
    "                if temp_verb:\n",
    "                    verb_phrase = \" \".join(temp_verb)\n",
    "                    verbs[verb_phrase] += 1\n",
    "                    temp_verb = []\n",
    "        \n",
    "        # Append remaining phrases after the loop\n",
    "        if temp_noun:\n",
    "            noun_phrase = \" \".join(temp_noun)\n",
    "            nouns[noun_phrase] += 1\n",
    "        if temp_adj:\n",
    "            adj_phrase = \" \".join(temp_adj)\n",
    "            adjectives[adj_phrase] += 1\n",
    "        if temp_verb:\n",
    "            verb_phrase = \" \".join(temp_verb)\n",
    "            verbs[verb_phrase] += 1\n",
    "\n",
    "    return adjectives, nouns, verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'phi35mini' # Select the model to analyze\n",
    "regions = ['EA','NE','LA','MEA','INDIA']\n",
    "\n",
    "if os.path.exists(f'{model}_pos_frequencies.json'):\n",
    "    with open(f'{model}_pos_frequencies.json') as f:\n",
    "        dict_data = json.load(f)\n",
    "        all_adjectives, all_nouns, all_verbs = dict_data['adjectives'], dict_data['nouns'], dict_data['verbs']# all_adjectives, all_nouns, all_verbs = {k:Counter() for k in regions}, {k:Counter() for k in regions}, {k:Counter() for k in regions}\n",
    "else:\n",
    "    all_adjectives, all_nouns, all_verbs = {k:Counter() for k in regions}, {k:Counter() for k in regions}, {k:Counter() for k in regions}\n",
    "    file_path = os.path.abspath(f'../../response/gen_bias/{model}')\n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(file_path, filename),encoding='utf-8')\n",
    "            for r in tqdm(regions):\n",
    "                if df[r].isna().all():\n",
    "                    continue\n",
    "                adjectives, nouns, verbs = extract_pos_single(df[r])\n",
    "                all_adjectives[r].update(adjectives)\n",
    "                all_nouns[r].update(nouns)\n",
    "                all_verbs[r].update(verbs)\n",
    "\n",
    "    def save_counters_to_json(adjectives, nouns, verbs, filename):\n",
    "        \"\"\"\n",
    "        Saves the parts of speech counters to a JSON file.\n",
    "        Args:\n",
    "            adjectives (Counter): Counter object for adjectives.\n",
    "            nouns (Counter): Counter object for nouns.\n",
    "            verbs (Counter): Counter object for verbs.\n",
    "            filename (str): Name of the output JSON file.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"adjectives\": dict(adjectives),\n",
    "            \"nouns\": dict(nouns),\n",
    "            \"verbs\": dict(verbs)\n",
    "        }\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Example usage\n",
    "    save_counters_to_json(all_adjectives, all_nouns, all_verbs, f\"{model}_pos_frequencies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{model}_pos_frequencies.json') as f:\n",
    "#     dict_data = json.load(f)\n",
    "#     all_adjectives, all_nouns, all_verbs = dict_data['adjectives'], dict_data['nouns'], dict_data['verbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_or(dict1,dict2,reg1,reg2):\n",
    "    \"\"\"\n",
    "    Calculate the odds ratio between two dictionaries of word frequencies.\n",
    "    Args:\n",
    "        dict1 (Counter): First dictionary of word frequencies.\n",
    "        dict2 (Counter): Second dictionary of word frequencies.\n",
    "        reg1 (str): Region name for the first dictionary.\n",
    "        reg2 (str): Region name for the second dictionary.\n",
    "    Returns:\n",
    "        dict: Dictionary containing the odds ratio for each word.\n",
    "    \"\"\"\n",
    "    words = set(dict1.keys()).union(set(dict2.keys()))\n",
    "    s1 = sum(dict1.values())\n",
    "    s2 = sum(dict2.values())\n",
    "    or_dict = {word:0 for word in words}\n",
    "    for word in words:\n",
    "        f1 = dict1[word] if word in dict1 else 0\n",
    "        f2 = dict2[word] if word in dict2 else 0\n",
    "        or_dict[word] = (f1/(s1-f1)+0.000001)/(f2/(s2-f2)+0.000001) # +0.000001 to avoid division by zero\n",
    "    return or_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_or = calculate_or(all_verbs['NE'],all_verbs['LA'],'NE','LA') # Enter two regions to compare model generations\n",
    "final_or = dict(sorted(initial_or.items(), key=lambda item: item[1],reverse=True)) # Sort the dictionary by values in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_one = {k:v for k,v in final_or.items() if v>1}\n",
    "less_one = {k:v for k,v in final_or.items() if v<=1}\n",
    "reg_more = {key:more_one[key] for key in random.sample(list(more_one.keys()), 2)}\n",
    "reg_less = {key:less_one[key] for key in random.sample(list(less_one.keys()), 20)}\n",
    "for k,v in reg_more.items():\n",
    "    print(\"{} ({:.4f})\".format(k.capitalize(),v))\n",
    "print()\n",
    "for k,v in reg_less.items():\n",
    "    print(\"{} ({:.4f})\".format(k.capitalize(),v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file for further analysis\n",
    "with open('temp.txt','w') as f:\n",
    "    for key in final_or.keys():\n",
    "        f.write(f'{key} : {final_or[key]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
