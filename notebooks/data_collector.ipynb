{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "from goose3 import Goose\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7184f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_readable(sentence):\n",
    "    \"\"\"\n",
    "    Check if the sentence is readable using Flesch Reading Ease score.\n",
    "    A higher score indicates easier readability.\n",
    "    Args:\n",
    "        sentence (str): The sentence to check.\n",
    "    Returns:\n",
    "        bool: True if the sentence is readable, False otherwise.\n",
    "    \"\"\"\n",
    "    score = textstat.flesch_reading_ease(sentence)\n",
    "    # Assuming a threshold of 30 for readability. Set by trial and error.\n",
    "    return 30 < score\n",
    "\n",
    "def scrape_website_with_goose3(url=None, input_text=None, output_file='scraped_content.xlsx'):\n",
    "    \"\"\"\n",
    "    Scrape a website using Goose3 and save the content to an Excel file.\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        input_text (str): The input text to process if no URL is provided.\n",
    "        output_file (str): The name of the output Excel file.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    g = Goose()\n",
    "    if url:\n",
    "        article = g.extract(url=url)\n",
    "        title = article.title\n",
    "        content = article.cleaned_text\n",
    "    elif input_text:\n",
    "        content = input_text  # If input text is provided, use it directly\n",
    "    else:\n",
    "        print(\"No URL or input text provided.\")\n",
    "        return\n",
    "\n",
    "    # Split the content into sentences\n",
    "    sentences = content.split('.')\n",
    "\n",
    "    # Create a new Excel workbook and sheet\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Content\"\n",
    "\n",
    "    # Initialize row number\n",
    "    row = 1\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and is_readable(sentence):  # Ensure the sentence is not empty and is readable\n",
    "            ws.cell(row=row, column=1, value=sentence)\n",
    "            row += 1\n",
    "\n",
    "    # Save the workbook to a file\n",
    "    wb.save(output_file)\n",
    "    print(f\"Content has been scraped and saved to {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "# scrape_website_with_goose3(url='https://en.wikipedia.org/wiki/Garri#Variations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5904b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(country_name, folder_path):\n",
    "    \"\"\"\n",
    "    Merge all Excel files for a specific country into one file.\n",
    "    Args:\n",
    "        country_name (str): The name of the country to filter files.\n",
    "        folder_path (str): The path to the folder containing the files.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the pattern to match files for the specific country\n",
    "    pattern = os.path.join(folder_path, f'{country_name}_*.xlsx')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found for {country_name}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize a list to hold dataframes\n",
    "    dataframes = []\n",
    "    \n",
    "    # Read each file and append the dataframe to the list\n",
    "    for file in files:\n",
    "        df = pd.read_excel(file,header=None)\n",
    "        dataframes.append(df)\n",
    "    # Concatenate all dataframes into one\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save the merged file\n",
    "    merged_file_path = os.path.join(folder_path+\"/Positive_Labels\", f'{country_name}.xlsx')\n",
    "    merged_df.to_excel(merged_file_path, index=False)\n",
    "    \n",
    "    # Optionally delete previous files\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "        print(f\"Deleted {file}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7806c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_sort_sentences_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Clean and sort sentences in all Excel files in the specified directory.\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing Excel files.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.xlsx'):\n",
    "            workbook_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing {workbook_path}...\")\n",
    "            clean_and_sort_sentences(workbook_path, 'Sheet1')  # Assuming 'Sheet1' is the sheet name\n",
    "\n",
    "def clean_and_sort_sentences(workbook_path, sheet_name,to = None,threshold = 0.8):\n",
    "    \"\"\"\n",
    "    Clean and sort sentences in the specified Excel file.\n",
    "    Args:\n",
    "        workbook_path (str): The path to the Excel file.\n",
    "        sheet_name (str): The name of the sheet to process.\n",
    "        to (str): The path to save the cleaned file. If None, overwrite the original file.\n",
    "        threshold (float): Threshold for sensibility check.\n",
    "    Returns:\n",
    "        int: The number of sentences processed.\n",
    "    \"\"\"\n",
    "    # Load the workbook and the specific sheet\n",
    "    wb = load_workbook(workbook_path)\n",
    "    sheet = wb[sheet_name]\n",
    "    file_name = workbook_path.split(\"/\")[-1]\n",
    "    if to is None:\n",
    "        to = workbook_path\n",
    "    else:\n",
    "        to = to+\"/\"+file_name\n",
    "    # Extract all sentences from the 1st column, skipping the header\n",
    "    sentences = []\n",
    "    for row in sheet.iter_rows(min_row=1, min_col=1, max_col=1):\n",
    "        cell_value = row[0].value\n",
    "        if cell_value and isinstance(cell_value, str):\n",
    "            sentence = cell_value.strip()\n",
    "            sentence = re.sub(r'^[\\u2022•*\\-><–.`\\'\\s]+', '', sentence)\n",
    "            sentence = re.sub(r'^[\\[\\(]\\d+[\\]\\)]\\s*', '', sentence)\n",
    "            sentence = sentence.replace('\\n', ' ')\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence)>20:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Remove empty rows and sort sentences by length\n",
    "    seen = set()\n",
    "    sentences = [x for x in sentences if not (x in seen or seen.add(x))]\n",
    "    sentences = sorted(filter(lambda x: x, sentences), key=len)\n",
    "    print(len(sentences))\n",
    "    # Clear the existing data in the 1st column\n",
    "    for row in sheet.iter_rows(min_row=1, min_col=1, max_col=1):\n",
    "        row[0].value = None\n",
    "    \n",
    "    # Write sorted sentences back to the 1st column\n",
    "    for index, sentence in enumerate(sentences, start=1):\n",
    "        sheet.cell(row=index, column=1).value = sentence\n",
    "    \n",
    "    # Save the workbook in place of the old one\n",
    "    wb.save(to)\n",
    "    print(f\"Workbook '{workbook_path}' has been cleaned and sorted.\")\n",
    "    return len(sentences)\n",
    "\n",
    "def remove_common_pair(read_from,check_from):\n",
    "    \"\"\"\n",
    "    Remove common pairs from two Excel files.\n",
    "    Args:\n",
    "        read_from (str): Path to the first Excel file.\n",
    "        check_from (str): Path to the second Excel file.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rd_data = pd.read_excel(read_from,header=None)\n",
    "    ch_data = pd.read_excel(check_from,header=None)\n",
    "    rd_data = rd_data[0].values\n",
    "    ch_data = ch_data[0].values\n",
    "    for i in range(len(rd_data)):\n",
    "        for j in range(len(ch_data)):\n",
    "            if rd_data[i] == ch_data[j]:\n",
    "                ch_data[j] = None\n",
    "    ch_data = [x for x in ch_data if x is not None]\n",
    "    df = pd.DataFrame(ch_data)\n",
    "    df.to_excel(check_from,index=False,header=False)\n",
    "\n",
    "def give_from_to(read_from,check_from):\n",
    "    \"\"\"\n",
    "    Append sentences containing 'india' from one Excel file to another.\n",
    "    This special pre processing function is required since data for India was scattered in other files (especially japan)\n",
    "    due to incorrect pre-processing in the earlier version of eticor.\n",
    "    Args:\n",
    "        read_from (str): Path to the first Excel file.\n",
    "        check_from (str): Path to the second Excel file.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rd_data = pd.read_excel(read_from,header=None)\n",
    "    ch_data = pd.read_excel(check_from,header=None)\n",
    "    rd_data = rd_data[0].values\n",
    "    ch_data = ch_data[0].values\n",
    "    print(len(rd_data),len(ch_data))\n",
    "    for i in range(len(rd_data)):\n",
    "        if 'india' in rd_data[i].lower():\n",
    "            ch_data = np.append(ch_data,rd_data[i])\n",
    "    df = pd.DataFrame(ch_data)\n",
    "    df.to_excel(check_from,index=False,header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc08ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls_from_file(txt_file,name=\"scraped_content\"):\n",
    "    \"\"\"\n",
    "    Scrape websites listed in a .txt file and save the content to separate Excel files.\n",
    "    Args:\n",
    "        txt_file (str): Path to the .txt file containing URLs.\n",
    "        name (str): Base name for the output Excel files.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the .txt file and read all the URLs\n",
    "    with open(txt_file, 'r') as file:\n",
    "        urls = file.readlines()\n",
    "\n",
    "    # Loop through each URL in the list\n",
    "    for i, url in enumerate(urls):\n",
    "        url = url.strip()  # Remove any leading/trailing whitespace/newline characters\n",
    "        if url:\n",
    "            try:\n",
    "                # Generate an output file name based on the URL or index\n",
    "                output_file = f\"{name}_{i+1}.xlsx\"\n",
    "                scrape_website_with_goose3(url=url, output_file=output_file)\n",
    "                print(f\"Content from {url} has been scraped and saved to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape {url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bbc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_country_files(directory):\n",
    "    \"\"\"\n",
    "    Process all .txt files in the specified directory.\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing .txt files.\n",
    "    Returns:\n",
    "        list: A list of country names extracted from the file names.\n",
    "    \"\"\"\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter the files to get only those matching the 'country.txt' pattern\n",
    "    txt_files = [f for f in files if f.endswith('.txt')]\n",
    "    country_names = [os.path.splitext(txt_file)[0] for txt_file in txt_files]\n",
    "    # Process each country file\n",
    "    for txt_file in txt_files:\n",
    "        country_name = os.path.splitext(txt_file)[0]  # Extract country name from the file name\n",
    "        txt_path = os.path.join(directory, txt_file)\n",
    "        \n",
    "        # Call scrape_urls_from_file function with the txt file and country name\n",
    "        scrape_urls_from_file(txt_path, country_name)\n",
    "    return country_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435cbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_common_pair(\"/home/siddhant_singh/lab_gpu/Positive_Labels/INDIA.xlsx\",\"/home/siddhant_singh/lab_gpu/Positive_Labels/JAP.xlsx\")\n",
    "give_from_to(\"/home/siddhant_singh/lab_gpu/Positive_Labels/JAP.xlsx\",\"/home/siddhant_singh/lab_gpu/Positive_Labels/INDIA.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b7e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NE.xlsx', 'INDIA.xlsx', 'LA.xlsx', 'MEA.xlsx', 'EA.xlsx']\n",
      "6607\n",
      "Workbook '/home/siddhant_singh/lab_gpu/Final_Positive_Labels/NE.xlsx' has been cleaned and sorted.\n",
      "2189\n",
      "Workbook '/home/siddhant_singh/lab_gpu/Final_Positive_Labels/INDIA.xlsx' has been cleaned and sorted.\n",
      "3487\n",
      "Workbook '/home/siddhant_singh/lab_gpu/Final_Positive_Labels/LA.xlsx' has been cleaned and sorted.\n",
      "6968\n",
      "Workbook '/home/siddhant_singh/lab_gpu/Final_Positive_Labels/MEA.xlsx' has been cleaned and sorted.\n",
      "5800\n",
      "Workbook '/home/siddhant_singh/lab_gpu/Final_Positive_Labels/EA.xlsx' has been cleaned and sorted.\n",
      "25051\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "file_paths = os.listdir(\"/home/siddhant_singh/lab_gpu/Final_Positive_Labels\")\n",
    "print(file_paths)\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        total_size += clean_and_sort_sentences(f\"/home/siddhant_singh/lab_gpu/Final_Positive_Labels/{file_path}\", \"Sheet1\",threshold = 0.5)\n",
    "    except:\n",
    "        total_size += clean_and_sort_sentences(f\"/home/siddhant_singh/lab_gpu/Final_Positive_Labels/{file_path}\", \"in\",threshold = 0.5)\n",
    "print(total_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
