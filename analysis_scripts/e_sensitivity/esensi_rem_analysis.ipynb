{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['acceptable', 'unacceptable']\n",
    "\n",
    "def extract_response(sent):\n",
    "    sent  = str(sent)\n",
    "    ret = []\n",
    "    if(sent == '' or sent==np.nan):\n",
    "        return ret\n",
    "    gen = sent.lower()\n",
    "    if options[1] in gen:\n",
    "        ret.append(options[1])\n",
    "    if len(ret):\n",
    "        return ret\n",
    "    if options[0] in gen:\n",
    "        ret.append(options[0])\n",
    "    if len(ret):\n",
    "        return ret\n",
    "    for i, opt in enumerate(options):\n",
    "        if chr(i + 1 + ord('0')) in gen:\n",
    "            ret.append(opt)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama31\" # Change this to the model response you want to analyze\n",
    "n_responses = 3\n",
    "analysis_array = np.zeros((2,7, n_responses))\n",
    "for response_number in range(1, n_responses + 1):\n",
    "    response_path = os.path.abspath(f\"../../response_{response_number}/esensi/{model}\")\n",
    "    response_files = os.listdir(response_path)\n",
    "    regions = [response_file.split(\"_\")[-1].split(\".\")[0] for response_file in response_files]\n",
    "\n",
    "    groups = [\"dining\",\"business\",\"visits\",\"travel\"]\n",
    "    analysis_dict = {region: {g:{\"total\":0,\"abstain\":0,\"wrong\":0,\"correct\":0} for g in groups} for region in regions}\n",
    "    true_label_dict = {region: {g:{\"total\":0,\"positive\":0,\"negative\":0} for g in groups} for region in regions}\n",
    "    response_rates = {region: {g:{\"true_positive\":0,\"false_positive\":0,\"true_negative\":0,\"false_negative\":0} for g in groups} for region in regions}\n",
    "\n",
    "    # Read the response files and calculate the data required for analysis\n",
    "    for file in response_files:\n",
    "        df = pd.read_csv(os.path.join(response_path, file))\n",
    "        region = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        for i in range(len(df)):\n",
    "            row = df.iloc[i]\n",
    "            grp = row[\"group\"]\n",
    "            true_label_dict[region][grp][\"total\"] += 1\n",
    "            if row[\"true_label\"]==\"positive\":\n",
    "                true_label_dict[region][grp][\"positive\"] += 1\n",
    "            else:\n",
    "                true_label_dict[region][grp][\"negative\"] += 1\n",
    "            response = extract_response(row[f'{model}_sentence'])\n",
    "            \n",
    "            if len(response)==0 or len(response)>1:\n",
    "                analysis_dict[region][grp][\"abstain\"] += 1\n",
    "            elif len(response)==1 and ((response[0]=='acceptable' and row[\"true_label\"]==\"positive\") or (response[0]=='unacceptable' and row[\"true_label\"]==\"negative\")):\n",
    "                analysis_dict[region][grp][\"correct\"] += 1\n",
    "                if response[0]=='acceptable' and row[\"true_label\"]==\"positive\":\n",
    "                    response_rates[region][grp][\"true_positive\"] += 1\n",
    "                else:\n",
    "                    response_rates[region][grp][\"true_negative\"] += 1\n",
    "            else:\n",
    "                analysis_dict[region][grp][\"wrong\"] += 1\n",
    "                if response[0]=='acceptable' and row[\"true_label\"]==\"negative\":\n",
    "                    response_rates[region][grp][\"false_positive\"] += 1\n",
    "                else:\n",
    "                    response_rates[region][grp][\"false_negative\"] += 1\n",
    "            analysis_dict[region][grp][\"total\"] += 1\n",
    "\n",
    "    # Calculate the total number of abstentions for a particular response_number\n",
    "    ab = 0\n",
    "    for region in regions:\n",
    "        for grp in groups:\n",
    "            ab += analysis_dict[region][grp][\"abstain\"]\n",
    "\n",
    "    analysis_array[0][-1][response_number-1] = ab\n",
    "    analysis_array[1][-1][response_number-1] = ab\n",
    "\n",
    "    results_acc = []\n",
    "    results_f1 = []\n",
    "    ord_regions = ['NE', 'INDIA', 'EA', 'LA', 'MEA']\n",
    "\n",
    "    # Calculate the accuracy and f1 score for each region along with their averages across regions.\n",
    "    for i,r in enumerate(ord_regions):\n",
    "        r = r.lower()\n",
    "        corrects = sum([analysis_dict[r][g][\"correct\"] for g in groups])\n",
    "        wrongs = sum([analysis_dict[r][g][\"wrong\"] for g in groups])\n",
    "        abstains = sum([analysis_dict[r][g][\"abstain\"] for g in groups])\n",
    "        total = corrects + wrongs + abstains\n",
    "        \n",
    "        true_positive_rate = sum([response_rates[r][g][\"true_positive\"] for g in groups]) / sum([true_label_dict[r][g][\"positive\"] for g in groups])\n",
    "        true_negative_rate = sum([response_rates[r][g][\"true_negative\"] for g in groups]) / sum([true_label_dict[r][g][\"negative\"] for g in groups])\n",
    "        false_positive_rate = sum([response_rates[r][g][\"false_positive\"] for g in groups]) / sum([true_label_dict[r][g][\"negative\"] for g in groups])\n",
    "        false_negative_rate = sum([response_rates[r][g][\"false_negative\"] for g in groups]) / sum([true_label_dict[r][g][\"positive\"] for g in groups])\n",
    "        \n",
    "        precision = sum([response_rates[r][g][\"true_positive\"] for g in groups]) / (sum([response_rates[r][g][\"true_positive\"] for g in groups]) + sum([response_rates[r][g][\"false_positive\"] for g in groups]))\n",
    "        recall = sum([response_rates[r][g][\"true_positive\"] for g in groups]) / (sum([response_rates[r][g][\"true_positive\"] for g in groups]) + sum([response_rates[r][g][\"false_negative\"] for g in groups]))\n",
    "        \n",
    "        print(f\"Region: {r}\")\n",
    "        print(f\"Accuracy: {corrects / total}\")\n",
    "        print(f\"f1 score: {2 * precision * recall / (precision + recall)}\")\n",
    "        results_acc.append(corrects / total)\n",
    "        results_f1.append(2 * precision * recall / (precision + recall))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(f\"Average Accuracy: {sum(results_acc) / len(results_acc)}\")\n",
    "    print(f\"Average F1 Score: {sum(results_f1) / len(results_f1)}\")\n",
    "\n",
    "    analysis_array[0][5][response_number-1] = sum(results_acc) / len(results_acc)\n",
    "    analysis_array[1][5][response_number-1] = sum(results_f1) / len(results_f1)\n",
    "\n",
    "    analysis_array[0][:5][response_number-1] = results_acc\n",
    "    analysis_array[1][:5][response_number-1] = results_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation across all responses for each region as well as average accuracy and f1 score.\n",
    "print(np.mean(analysis_array[0], axis=2))\n",
    "print(np.mean(analysis_array[1], axis=2))\n",
    "print(np.std(analysis_array[0], axis=2))\n",
    "print(np.std(analysis_array[1], axis=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
